# VESTA — The Core Idea

Every Vision-Language Model today has the same blind spot: **spatial amnesia**. You can point a camera at a construction site, ask Gemini or GPT "what hazards do you see?", and it'll give you a decent answer — for that one frame. Pan the camera 90 degrees to the right, ask "what was behind me?", and it has no idea. The hazard it literally just saw three seconds ago is gone from its memory. Every frame is a blank slate.

VESTA fixes this by putting a **stateful hazard registry** between the camera and the AI. The system works in a continuous loop: every ~30 frames, we send a keyframe to Gemini Flash, which detects hazards and returns their pixel coordinates. Between those keyframes, we run OpenCV optical flow on every frame to track exactly how the camera rotated and translated. Here's the key insight — when Gemini spots a "floor hole" at the right edge of the frame, we don't just store "floor hole at pixel (1400, 600)." We convert that pixel position into a **world-fixed polar angle** (e.g., 45° to the right of where the worker was originally facing) using the camera's current heading. That angle lives in a persistent Python dictionary — the registry — and it never gets thrown away. As the worker turns their head, optical flow tells us "the camera rotated 30° left this second," so we update the heading, but the hazard's world angle stays fixed. Ten seconds and a full 180° turn later, we can do simple subtraction — the hole that was at 45° right is now at 135° behind-right — and answer "what's behind me?" with perfect accuracy, even though the AI hasn't seen that hole in hundreds of frames. The hazard became a "ghost" on the map: invisible to the camera, but still known to the agent.
